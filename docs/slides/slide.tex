\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{listings} %for listings
\usepackage{xcolor}

%colorini
\definecolor{comments}{rgb}{0.57, 0.64, 0.69} %commenti
\definecolor{codegray}{rgb}{0.5,0.5,0.5} %codice normale
\definecolor{strings}{rgb}{0,0.6,0} %stringhe
\definecolor{backcolour}{rgb}{0.94, 0.97, 1.0} %sfondo
\definecolor{pythonfunc}{rgb}{0.9, 0.17, 0.31}

%custom listings
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{comments},
    keywordstyle=\color{pythonfunc},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{strings},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
%set the custom listings
\lstset{style=mystyle}



\usetheme{Madrid}
\usecolortheme{default}

%------------------------------------------------------------
%This block of code defines the information to appear in the
%Title page
\title[Cloud Atlas] %optional
{Cloud Atlas}

\subtitle{An LstmEncoder for UHECR AirShowers}

\author[Gianluca Becuzzi, Lucia Papalini] % (optional)
{G. Becuzzi \and L. Papalini}

\date[July 2022] % (optional)
{July 2022}

%End of title page configuration block
%------------------------------------------------------------



%------------------------------------------------------------
%The next block of commands puts the table of contents at the 
%beginning of each section and highlights the current section:

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection]
  \end{frame}
}
%------------------------------------------------------------


\begin{document}

%The next statement creates the title page.
\frame{\titlepage}


%---------------------------------------------------------
%This block of code is for the table of contents after
%the title page
\begin{frame}
\frametitle{Table of Contents}
\tableofcontents
\end{frame}
%---------------------------------------------------------


\section{Introduction}

%---------------------------------------------------------
\begin{frame}{UHECR Airshower}

    \begin{columns}
    
    \column{0.5\textwidth}
    When \textit{Ultra High Energy Cosmic Rays} (UHECR) enters the atmosphere they produce a particle cascade.\\
    \vspace{15 pt}
    
    \textbf{Detection}: grid of water-Cherenkov ground based detectors.
    \vspace{10 pt}
    
    \textbf{Prediction}: $X_0$ height at which the shower forms.
    
    
    \column{0.5\textwidth}
        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{airshower.png}
            \label{fig:my_label}
        \end{figure}
        
    
    \end{columns}
\end{frame}
%---------------------------------------------------------


%---------------------------------------------------------
\begin{frame}{Dataset, first glance}

    The dataset is composed of $10^5$ simulated events:

    \begin{itemize}
        \item $9 x 9$ grid of detectors
        \item most intense detector at the center 
        \item 80 frames of time series ($40$ $M$Hz sampling rate)
        \item 1 frame of times of first arrival 
    \end{itemize}

    The single record shape is then $(80 + 1 , 81)$

    The \texttt{pd4ml} package splits by default in $70\%$ train $30\%$ test.

\end{frame}

%---------------------------------------------------------

\section{Preprocessing}

%---------------------------------------------------------
\begin{frame}{Split the dataset}
    Using a generator (\texttt{keras.utils.Sequence})
    \begin{itemize}
        \item inherit multiprocessing features
        \item has default callbacks
    \end{itemize}
    The dataset is splitted \emph{record by record} for index shuffling

    The effect of the high reading time from memory ($\approx 3 m$s) is mitigated
    by \texttt{keras} multiprocessing
    
    For the design of the net it is convenient using \texttt{numpy} structured arrays
\end{frame}

%---------------------------------------------------------
\begin{frame}[fragile]{Split the dataset: \texttt{funky\_dtype}}

    
    \begin{lstlisting}[language=Python]
    # custom numpy dtype
    funky_dtype = np.dtype(
        [
            ("outcome", np.float64),
            ("time_series", np.float32, (80, 81)),
            ("toa", np.float32, (9, 9, 1)),
        ])
    \end{lstlisting}
    

    Data is extracted: from a conceptually \emph{ihomogeneous} list 
    (activity time series together with times of arrival) to
    $(80 + 1, 81) \rightarrow [("toa", (9, 9, 1)), ("timeseries", (80, 9,9))]$

    Data can be accessed depending on what is needed

\end{frame}


%---------------------------------------------------------
\begin{frame}{DataFeeder class}
    Ensures an easy way to train the subnets separately

    \begin{itemize}
        \item shuffles data randomly
        \item input fields can be specified
        \item can be extended to more complex training strategies
    \end{itemize}
    
\end{frame}

%---------------------------------------------------------
\begin{frame}{DataFeeder class}{Curriculum learning}

    Using a pre-trained network data can be ``scored'' in ascending order of difficulty

    (work in progress) This can lead to a learning speed-up and improvements in resolution
    
    Caveat: this training strategy is not well suited (conceptually at least) for regression tasks, 
    since it is not clear what a ``difficult'' sample would look like.
\end{frame}

%---------------------------------------------------------
\begin{frame}{Data Augmentation}
    Dataset has a lack of high events ($X > 850$m) so the network resolution is worse
    for samples corresponding to this range

    \begin{block}{Strategy}
        Increase the number of samples conditionally on event heigth using the
        symmetries of the problem
    \end{block}

    Data is augmented using
    \begin{itemize}
        \item flip up-down
        \item flip left right
        \item rotation of $90^{\circ}$
    \end{itemize}
    
    It must me higlighted that only a subset of the available data undergoes this procedure.

    Augmenting the whole dataset would leave the sample distribution unchanged and thus would not lead 
    to improvements.
\end{frame}

\begin{frame}{Resolution}

    The reference article suggests using the resolution:
    \begin{block}{Resolution}
        defined as the standard deviation of the distribution given by the difference between the predictions and the actual values of $X_{max}$
    \end{block}

    We point out that 
    \[\sigma^2 = \frac{1}{N}\sum_i (\delta_i - \bar{\delta})^2\]
    is a sensible estimator of ``how much the net has gone wrong'' only if $\bar{\delta} = 0$, for which the adopted resolution is equal 
    to the $RMSE$ of the distribution
    \[ RMSE^2 = \frac{1}{N}\sum_i(x_i - \hat{x}_i)^2 \]
    Since (on a typical train) $\bar{\delta} \approx 10$m we preferred the RMSE.
\end{frame}



\section{Neural Network building}

    \centering
    \includegraphics[width=0.6\textwidth]{model.pdf}
%---------------------------------------------------------
\begin{frame}{Overview on the network}
    The assumption that lead to this design is that from the time of arrival matrix
    it is possible to infer some kind of ``homogeneous'' shower parameters (incidence angle, spread, etc.)
    while the time series can be processed by a recurrent network.
\end{frame}

%---------------------------------------------------------
\begin{frame}{Encoder for time of arrivals}

    \includegraphics[width=\textwidth]{ENC_history.png}
    
\end{frame}


%---------------------------------------------------------
\begin{frame}{LSTM}
si spiega che cos'Ã¨
    
\end{frame}

%---------------------------------------------------------
\begin{frame}{LSTM for the time series}
    \includegraphics[width=\textwidth]{LSTM_history.png}    
\end{frame}

\begin{frame}{Subnets performance}
    \includegraphics[width=\textwidth]{sub_net_train.pdf}
\end{frame}

%---------------------------------------------------------
\begin{frame}{Concatente + dense layers}

    
\end{frame}


\begin{frame}{Subnets train freezing}
    \includegraphics[width=.8\linewidth]{freezetraining_2.pdf}
\end{frame}

\begin{frame}{Subnets train freezing}
    \includegraphics[width=.8\linewidth]{freezetraining.pdf}
\end{frame}

%---------------------------------------------------------
\begin{frame}[fragile]{Network's output}
    \begin{lstlisting}[language=Python]
    import numpy as np
        
    def incmatrix(genl1,genl2):
        m = len(genl1)
        n = len(genl2)
        M = None #to become the incidence matrix
        string = "ciao"
      
    \end{lstlisting}

    
\end{frame}

%---------------------------------------------------------
\begin{frame}{Hyperparameters tuning}

    
\end{frame}


%---------------------------------------------------------
\begin{frame}{Whole Network performance}

    
\end{frame}

\begin{frame}{Test setup on CircleCI}

    
\end{frame}

%---------------------------------------------------------
\begin{frame}{Danke e bibliography}
\centering
Danke Schon

    
\end{frame}




\end{document}